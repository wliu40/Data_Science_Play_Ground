{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how a decision tree split\n",
    "gini index\n",
    "information gain\n",
    "\n",
    "\n",
    "\n",
    "Random forest, \n",
    "\n",
    "gradient boosting, \n",
    "\n",
    "XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "1. How to build a decision tree? First pick the root of the tree, we compute the gini index(gini impurity) for all the attributes, and pick the attributes wich can make the minimum weighted gini index as the root. Then we recurvisvely use this method, and pick the rest split for the left and right child of the root. We will stop if no available attributes, or further split will not decrease the gini impurity.\n",
    "2. How to split with continuous values? We will order the numerical values, and for each pair, compute the mean value of them. Then we will have n-1 values from n samples. Use the value < or > a particular value for split.\n",
    "3. How to split with more then two attributes. We will make all the possible combination of those values. For example, color of red, yellow and green. The questions are: do you like red, do you like yellow, do you like green, do you like red or green, and etc...\n",
    "\n",
    "#### what is the split criterion for split?\n",
    "\n",
    "Let's compare RandomForestClassifier vs RandomForestRegressor\n",
    "in RandomForestClassifier:\n",
    "criterion{“gini”, “entropy”}, default=”gini”\n",
    "in RandomForestRegressor:\n",
    "criterion{“squared_error”, “absolute_error”, “poisson”}, default=”squared_error”\n",
    "\n",
    "### name some of the important parameters for tuneing the RF.\n",
    "\n",
    "1. n_estimators, int, how many trees will be built?\n",
    "2. max_depth. int, The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "3. max_features, int, The number of features to consider when looking for the best split.\n",
    "3. criterion, str, see above.\n",
    "4. boostrap, boolean, Whether bootstrap samples are used when building trees?\n",
    "5. min_samples_leaf, int, The minimum number of samples required to be at a leaf node.\n",
    "6. min_impurity_decrease, float, A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap aggregation\n",
    "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
    "\n",
    "Given a standard training set with size n, bagging generates m new training sets, each of size n′, by sampling from D uniformly and with replacement. By sampling with replacement, some observations may be repeated in each set. If n′=n, then for large n, the set is expected to have the fraction (1 - 1/e) (≈63.2%) of the unique examples of D, the rest being duplicates. This kind of sample is known as a bootstrap sample. \n",
    "why Sampling with replacement?\n",
    "To ensures each bootstrap is independent from its peers, as it does not depend on previous chosen samples when sampling. \n",
    "Then, m models are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adaboost differences with random forest\n",
    "1. adaboost use forest stumps rather than trees (stump is a special tree with only 1 node). \n",
    "in random forest, the order of the trees doesnot matter, but in the forst stumps built with adaboost, the order matters.\n",
    "\n",
    "2. The error that the first stump make will effect the way how the second stump made.  Each stump will take into the previous stump's error into account.\n",
    "\n",
    "3. In a random forest, each tree has the same weight when voting, but adaboost built stump forest, different stump has different\n",
    "weight.\n",
    "\n",
    "### --------------\n",
    "The following describes how adaboost works.\n",
    "\n",
    "1. First, you will assign each sample a weight. Intially, all samples will have the same weight. For example, if you have 8 samples in your trainning dataset, you will assign 1/8=0.125 to each sample.\n",
    "2. You will choose the first stump base on the gini index. Say you have three features. You will build three stumps and compute the gini index for each stump. Then you pick the stump which has minimum gini index as the first stump.\n",
    "3. You will compute how much say the stump has, based on how well this stump classify the samples. The total error = the sum of the sample weights which have wrong predicitons. Assume your first stump make 1 error on sample x. so in this case, the total error is 0.125. The amount of say = 0.5*log((1-total_error)/total_error). If a stump has prediction power as bad as random guess, the amount of say will be zero. If a stump try to make opposite prediction (worse than random guess), then the amount of say will be negative value.\n",
    "4. After the first stump used, the sample weight for the incorrect classified sample will be increased, all the other correctly classified sample's sample weight will be decreased.\n",
    "The new sample weight for the wrong predicted sample will be: curret_sample_weight * exp(last_used_stump's_amount_of_say). So, if the previous stump did a good job, then the amount of say will be large, and the sample weight will be increased large. If the previous stump did a poor job and its amount of say is small, then the sample weight will not be increased too much.\n",
    "The new sample weight for the correct predicted sample will be: curret_sample_weight * exp(-last_used_stump's_amount_of_say).\n",
    "\n",
    "5. After the step 4, each sample has a new sample weight. You will have to normalized the sample weights, so that the sum of all the sample weights can be added up to 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient bosst Regression Trees\n",
    "Like adaboost, the gradient boost will build trees sequentially, the next tree was built based on the errors that the previous tree has made..But unlike adaboost, the gradient boost will build trees that usually larger than a stump.\n",
    "#### How to build a gradient boost regression tree step by step?\n",
    "For example, we want to build a gradient boost regression tree to predict some continuous values from several categorical attributes.\n",
    "1. You start with a single leaf, which is the average of all the values.\n",
    "2. then, you caculate the 'pseudo residules' based on the avarage values, and you will build the next tree to predict this 'pseudo residua'. After the tree was built, you will need to scale the tree with a learning rate.\n",
    "3. like the last time, you will recompute the 'pseudo residual' and build the next tree...\n",
    "4. we will rescale all the trees with the same learning rate. Each tree will make sure you move toward the right direction a small step.\n",
    "5. Suppose you already have 4 trees, so now the residuals will be the initial prediction of first leaf(average value) + lr*(the_first_tree_predicted_residuals) + lr*(the_second_tree_predicted_residuals) + ...\n",
    "the residuals getting smaller and smaller with this process, \n",
    "6. we keep adding trees, until the number of trees meets our goal, or adding the next tree won't significantly reduce the residuels...\n",
    "\n",
    "#### sklearn implementation\n",
    "sklearn.ensemble.GradientBoostingRegressor\n",
    "1. loss{‘squared_error’, ‘absolute_error’, ‘huber’, ‘quantile’}, default=’squared_error’\n",
    "2. learning_ratefloat, default=0.1\n",
    "3. n_estimators\n",
    "4. criterion{‘friedman_mse’, ‘squared_error’, ‘mse’, ‘mae’}, default=’friedman_mse’\n",
    "5. max_features{‘auto’, ‘sqrt’, ‘log2’}, int or float, default=None\n",
    "6. max_depth, max_depth of a tree\n",
    "7. subsamples, The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "#### some references\n",
    "XGBoost parameters tuning\n",
    "1.https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "and:\n",
    "\n",
    " https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n",
    " \n",
    " https://www.youtube.com/watch?v=dMulLZKm_pg\n",
    "\n",
    "XGBoost with pyspark\n",
    "2.https://databricks.com/blog/2020/11/16/how-to-train-xgboost-with-spark.html\n",
    "\n",
    "XGBoost vs lightgbm vs catboost\n",
    "3.https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/\n",
    "\n",
    "#### Advantages of xgboost versus Gradient boost trees\n",
    "1. XGBoost has regularization terms, which can pervent overfitting.\n",
    "2. XGBoost can be paralled, it can be trained on cpu as well as on gpu.\n",
    "3. XGBoost has built-in cross validation method, like keras training. Thus you can monitor the real-time loss on validation dataset.\n",
    "4. XGBoost models can be saved to json or txt format. Thus, you can continue training if you have new data.\n",
    "\n",
    "\n",
    "#### How to build a xgboost tree for regression?\n",
    "\n",
    "1. like gradient boost regression, we start with the mean value of all the ys.\n",
    "2. instead of using off-shelf decision tree, xgboost use a special desion tree.\n",
    "3. we will compute the similarity score = sum of residual square / (number of residual + lambda), lambda is a reguarlazation term. we will pick the feature, which can decrease the similarity score. The gain = left_similarity_score + right_similarity_score - root_similarity_score. in this way, we will group the residues, make sure similar residules will end up in the same group.\n",
    "4. we use gamma to prune the tree, if the gain is less then the gamma, we will remove the branch.\n",
    "5. eta is the learning rate.\n",
    "6. alpha and beta are the values for L1 and L2 regurarization.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost hyperparameters\n",
    "\n",
    "\n",
    "https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble methods\n",
    "\n",
    "#### stacking, use heterogeneous models\n",
    "###### sklearn example\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "estimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "\n",
    "            ('svr', make_pipeline(StandardScaler(),LinearSVC(random_state=42)))]\n",
    "                  \n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "#### bagging(bootstrap aggregating), for example, random forest\n",
    "\n",
    "#### boosting, for example, adaboost, gradient boost, xgboost, and etc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
