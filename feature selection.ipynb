{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection\n",
    " https://www.youtube.com/watch?v=YaKMeAlHgqQ\n",
    " https://www.kaggle.com/prashant111/comprehensive-guide-on-feature-selection\n",
    "\n",
    "chi-squared test\n",
    "https://www.jianshu.com/p/807b2c2bfd9b\n",
    "\n",
    "VIF / variance-inflaction-factor\n",
    "https://www.statisticshowto.com/variance-inflation-factor/\n",
    "\n",
    "1. EDA, remove constant features, or features that have samll variance, or features that have lots of missing values, use sklearn varianceThreshold method\n",
    "2. use random forest, evaluate the feature importance. \n",
    "Feature importances are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree.\n",
    "3. perform chi-squred test, if no obvious dependency bwt the in-dependent variable and dependent variable, drop it.\n",
    "4. use anto-encoder(it can be viewed as just a nonlinear extension of PCA.)\n",
    "5. fit a classifier with L1 regularization.\n",
    "6. For multicollinearity, do VIF. remove higher VIF variables.\n",
    "\n",
    "VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.\n",
    "\n",
    "VIF starts at 1 and has no upper limit\n",
    "VIF = 1, no correlation between the independent variable and the other variables\n",
    "VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n",
    "\n",
    "\n",
    "\n",
    "7. backward elimination, forward selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  how to handle outliers in a linear regression model\n",
    "\n",
    "Winsorizing: This method involves setting the extreme values of an attribute to some specified value. \n",
    "    E.g., set the < 5percentile data to the min value at the 5th percentile\n",
    "    \n",
    "log-transfromation : only prefered for right skewed dataset\n",
    "\n",
    "rank-transformation :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 and R2-adjust\n",
    "\n",
    "R2 ï¼š R-squared = (TSS-RSS)/TSS = Explained variation/ Total variation. Add a useless/redudant variable to the model will not decrease the R2 value.\n",
    "R2-adjust, will takes into account the number of independant variables and also the number of instaces.\n",
    "In doing so, we can determine whether adding new variables to the model actually increases the model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding\n",
    "\n",
    "#### onehot encoding\n",
    "\n",
    "#### label encoding\n",
    "\n",
    "\n",
    "#### frequency encoding\n",
    "\n",
    "\n",
    "#### mean encoding/target encoding/likelihood encoding\n",
    "\n",
    "https://datascience.stackexchange.com/questions/11024/encoding-categorical-variables-using-likelihood-estimation\n",
    "\n",
    "\n",
    "https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b\n",
    "\n",
    "https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "# T-test, p-values\n",
    "# standard error standard deviate of sampling distribution of mean\n",
    "# latent space\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow and keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoders\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge distilling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stepwise regression\n",
    "\n",
    "# Backward elimination\n",
    "import statsmodels.formula.api as sm\n",
    "def backwardElimination(x, Y, sl, columns):\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(Y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    columns = np.delete(columns, j)\n",
    "                    \n",
    "    regressor_OLS.summary()\n",
    "    return x, columns\n",
    "SL = 0.05\n",
    "data_modeled, selected_columns = backwardElimination(data.iloc[:,1:].values, data.iloc[:,0].values, SL, selected_columns)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when to use ridge(l2) vs lasso(l1)?\n",
    "# what is the downside of using normalization?\n",
    "# standardize vs normalize the input dataset\n",
    "# what if features more than your training size?\n",
    "# why must normalize data if using regularization?\n",
    "# what is r2=0 mean? what if r2 = 1 mean?\n",
    "# why ordinary least square stands for?\n",
    "# 5 differnet loss functions in linear regression models\n",
    "# https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "# what is c and gamma in svc?\n",
    "# what is the pros and cons of svm? is svm good for big dataset or high dimension?\n",
    "# cross-validation is not used for model-tuning, but for model evaluation\n",
    "# use grid-search for model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALS for recommender system\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-1-knn-item-based-collaborative-filtering-637969614ea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
